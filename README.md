# Image Colorization with Conditional GANs (Pix2Pix)

### ğŸ¯ Project Overview

This project addresses **automatic image colorization** of grayscale bird images using the *20 UK Garden Birds* dataset from Kaggle.
We implemented and compared two approaches:

1. **Naive CNN Autoencoder** â€“ a supervised baseline for grayscaleâ†’RGB translation.
2. **Conditional GAN (Pix2Pix)** â€“ using a **U-Net generator** for stable, fine-grained generation and a **PatchGAN discriminator** for local realism.

The goal was to evaluate how **adversarial training** improves realism and sharpness compared to traditional supervised models.

---

## ğŸ“š Research Context

The project builds on the paper

> *Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks" (CVPR 2017)*

Pix2Pix introduced the idea of **conditional GANs (cGANs)** for pixel-to-pixel mapping tasks such as grayscaleâ†’color, edgesâ†’photo, etc.
Its loss combines a **GAN component** (realism) with an **L1 reconstruction term** (structural accuracy):
[
L = L_{cGAN}(G,D) + \lambda L_{1}(G)
]
where **Î» = 100** balances color consistency and sharpness.

---

## ğŸ§  Implementation Details

### Dataset

* **Source:** [20 UK Garden Birds](https://www.kaggle.com/datasets/davemahony/20-uk-garden-birds)
* **Input:** Grayscale image (128Ã—128Ã—1)
* **Output:** RGB image (128Ã—128Ã—3)
* **Split:** 80% train, 10% validation, 10% test
* **Preprocessing:** resizing, normalization (0â€“1 or âˆ’1â€“1), controlled augmentation (flips, rotations)

---

### Architectures

#### ğŸ§© 1. Naive CNN Autoencoder

* Encoderâ€“decoder CNN
* Experiments:

  * **Exp.1:** Baseline (simple CNN)
  * **Exp.2:** + Dropout (reduce overfitting)
  * **Exp.3:** + BatchNorm & deeper network (improved stability and color accuracy)
* **Loss:** MSE   |   **Optimizer:** Adam (1e-3)

#### âš¡ 2. Conditional GAN (Pix2Pix)

* **Generator:** U-Net (encoder-decoder with skip connections)
* **Discriminator:** PatchGAN (70Ã—70 local classifier)
* **Loss:** L1 + GAN (Î» = 100)
* **Optimizer:** Adam (2e-4, Î²1 = 0.5)
* **Experiments:**

  * **Exp.1:** Baseline Pix2Pix (L1 + GAN)
  * **Exp.2:** MobileNetV2 encoder integration (partial unfreezing â‰ˆ20 layers)
  * **Exp.3:** Tuned Î» values (80â€“120) to balance realism vs. consistency

---

## ğŸ§ª Experiments and Results

| Model                     | Key Techniques       | Best Epoch | Observations                          |
| :------------------------ | :------------------- | :--------: | :------------------------------------ |
| CNN Baseline              | Simple Autoencoder   |     20     | Low-saturation colors, underfitting   |
| CNN + Dropout             | Regularization (0.2) |     50     | Less overfitting, slightly blurry     |
| CNN + BatchNorm (deeper)  | Stable training      |     70     | Sharper colors, best CNN result       |
| Pix2Pix (L1 + GAN, Î»=100) | U-Net + PatchGAN     |     50     | Strong color realism, minor artifacts |
| Pix2Pix + MobileNetV2     | Transfer learning    |    100+    | Richer colors, risk of overfitting    |

**Performance Summary**

* BatchNorm CNN â†’ best supervised result.
* Pix2Pix (Î»=100) â†’ most vivid and realistic outputs.
* MobileNetV2 encoder enhanced texture detail but required careful fine-tuning.
* Qualitative evaluation (output images) was more informative than numeric loss values.

---

## ğŸ§© Notebook Structure

```
â”œâ”€â”€ Notebook 1 â€“ Naive_CNN_Colorization.ipynb
â”‚   â”œâ”€â”€ Data loading & preprocessing
â”‚   â”œâ”€â”€ 3 CNN experiments (baseline â†’ BatchNorm)
â”‚   â””â”€â”€ Loss curves + generated outputs
â”‚
â”œâ”€â”€ Notebook 2 â€“ Pix2Pix_CGAN_Colorization.ipynb
â”‚   â”œâ”€â”€ U-Net & PatchGAN implementation
â”‚   â”œâ”€â”€ 3 CGAN experiments (Î», MobileNetV2)
â”‚   â””â”€â”€ Visual comparisons & training plots
â”‚
â””â”€â”€ Notebook 3 â€“ Test_Environment.ipynb
    â”œâ”€â”€ Load best weights
    â”œâ”€â”€ Inference on new images
    â””â”€â”€ Display side-by-side results
```

---

## âš™ï¸ Setup & Execution

1. **Environment**

   * Python â‰¥ 3.9
   * TensorFlow/Keras â‰¥ 2.10
   * NumPy, Matplotlib, gdown, PIL

2. **Run in Google Colab**

   * Mount Google Drive
   * Download dataset and weights (via `gdown`)
   * Execute cells sequentially

3. **Inference**

   * Upload a 128Ã—128 grayscale image
   * Model outputs its colorized version

---

## ğŸ’¡ Insights & Future Work

* **L1 vs. GAN balance:** Î»â‰ˆ100 offered the best trade-off between global color accuracy and sharpness.
* **Skip connections:** Improved luminance preservation and detail fidelity.
* **Transfer learning:** MobileNetV2 encoder added semantic context but required layer-freezing control.
* **Next steps:** Try perceptual loss (VGG), larger datasets, and Vision Transformer backbones for richer color semantics.

---

## ğŸ‘¥ Team

**Roy Edri**, **Nati Forish**, **Iyar Gadulov**, **Inon Elgabsi**
*Advanced Topics in Deep Learning â€“ Final Project (2025)*
